{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 需要注意读取的路径不能有中文的字符\n",
    "- 目前的问题是用的是cmax，需要重新选择相应的公式  \n",
    "- 目前已经成功获得了10%均匀采样的数据\n",
    "- 注意pytho2和python3的区别，除法的结果\n",
    "- tensor.view里面的-1表示infer的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(data_array: np.array):\n",
    "    \"\"\"\n",
    "    输入二维的array，输出相应的面积图片\n",
    "    这里人为定义了vmax的数值，之后可能需要改改\n",
    "    \"\"\"\n",
    "    shape = data_array.shape\n",
    "    x = np.arange(0, shape[1])  # len = 11\n",
    "    y = np.arange(0, shape[0])  # len = 7\n",
    "\n",
    "    fig, ax = plt.subplots(dpi=150)\n",
    "    pcm = ax.pcolormesh(x, y, data_array, vmax=data_array.max(), vmin=0, cmap=\"Blues\")\n",
    "    fig.colorbar(pcm, ax=ax)\n",
    "\n",
    "\n",
    "# plot_distribution(O3_con)"
   ]
  },
  {
   "source": [
    "# fix the data size problem"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "O3_hourly_32_sh\n",
      "O3_hourly_32_sh_sample_by_station\n",
      "O3_hourly_32_sh_sample_by_station_test\n",
      "O3_hourly_32_sh_test\n",
      "PM25_hourly_32_sh\n",
      "PM25_hourly_32_sh_sample_by_station\n",
      "PM25_hourly_32_sh_sample_by_station_test\n",
      "PM25_hourly_32_sh_test\n"
     ]
    }
   ],
   "source": [
    "base = \"data/data-final-copy/\"\n",
    "for folder in os.listdir(base):\n",
    "    names = os.listdir(base+folder)\n",
    "    print(folder)\n",
    "    for name in names:\n",
    "        path = base + folder +\"/\" + name\n",
    "        arr = np.load(path, allow_pickle=True)\n",
    "        if arr.shape[1] == 34:\n",
    "            arr = arr[:, 2:]\n",
    "        np.save(path, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "O3_hourly_32_sh\n",
      "O3_hourly_32_sh_sample_by_station\n",
      "O3_hourly_32_sh_sample_by_station_test\n",
      "O3_hourly_32_sh_test\n",
      "PM25_hourly_32_sh\n",
      "PM25_hourly_32_sh_sample_by_station\n",
      "PM25_hourly_32_sh_sample_by_station_test\n",
      "PM25_hourly_32_sh_test\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "base = \"data/data-final-copy/\"\n",
    "for folder in os.listdir(base):\n",
    "    names = os.listdir(base+folder)\n",
    "    print(folder)\n",
    "    for name in names:\n",
    "        path = base + folder +\"/\" + name\n",
    "        arr = np.load(path, allow_pickle=True)\n",
    "        if arr.shape != (32,32):\n",
    "            print(\"error on path: {}\".format(path))\n",
    "print(\"success\")"
   ]
  },
  {
   "source": [
    "# split the dataset to 90% training and 10% testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finished\n",
    "path = \"data/data-final-copy/PM25_hourly_32_sh_sample_by_station\"\n",
    "os.mkdir(path+'_test')\n",
    "\n",
    "names = pd.Series(os.listdir(path))\n",
    "train_list = names.sample(frac=0.9, random_state=42)\n",
    "test_list = set(names) - set(train_list)\n",
    "print(\"the len of the test_list is {}\".format(len(test_list)))\n",
    "for name in test_list:\n",
    "    # shutil.move(path+'/'+name, path+'_test'+'/'+name)\n",
    "    print(\"moved \"+name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data\n",
    "path = \"data/data-final-copy/O3_hourly_32_sh_sample_by_station_test\"\n",
    "arr = np.load(path + \"/\" + os.listdir(path)[0], allow_pickle=True)\n",
    "plot_distribution(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/data-final-copy/O3_hourly_32_sh\"\n",
    "arr = np.load(path + \"/\" + os.listdir(path)[0], allow_pickle=True)\n",
    "plot_distribution(arr)"
   ]
  },
  {
   "source": [
    "# change the image size to 32*32"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check result\n",
    "path = 'data/O3_hourly_32_sh/'\n",
    "arr = np.load(path+os.listdir(path)[10], allow_pickle=True)\n",
    "plot_distribution(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/PM25_hourly_sample_by_station/'\n",
    "arr = np.load(path+os.listdir(path)[0], allow_pickle=True)\n",
    "plot_distribution(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = 'data/O3_hourly/'\n",
    "save_folder = 'data/O3_hourly_32_sh/'\n",
    "# os.mkdir(save_folder)\n",
    "names = os.listdir(base_folder)\n",
    "for i, name in enumerate(names[:5]):\n",
    "    if (i%500 == 0): print('processed: {}/{}'.format(i, len(names)))\n",
    "    arr = np.load(base_folder+name, allow_pickle=True)[65:97, 196:228]\n",
    "    # np.save(save_folder+name, arr)\n",
    "    plot_distribution(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = 'data/O3_hourly_sample_by_station/'\n",
    "save_folder = 'data/O3_hourly_32_sh_sample_by_station/'\n",
    "os.mkdir(save_folder)\n",
    "names = os.listdir(base_folder)\n",
    "for i, name in enumerate(names):\n",
    "    if (i%500 == 0): print('processed: {}/{}'.format(i, len(names)))\n",
    "    arr = np.load(base_folder+name, allow_pickle=True)[65:97, 194:228]\n",
    "    np.save(save_folder+name, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = 'data/PM25_hourly/'\n",
    "save_folder = 'data/PM25_hourly_32_sh/'\n",
    "os.mkdir(save_folder)\n",
    "names = os.listdir(base_folder)\n",
    "for i, name in enumerate(names):\n",
    "    if (i%500 == 0): print('processed: {}/{}'.format(i, len(names)))\n",
    "    arr = np.load(base_folder+name, allow_pickle=True)[65:97, 194:228]\n",
    "    np.save(save_folder+name, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = 'data/PM25_hourly_sample_by_station/'\n",
    "save_folder = 'data/PM25_hourly_32_sh_sample_by_station/'\n",
    "os.mkdir(save_folder)\n",
    "names = os.listdir(base_folder)\n",
    "for i, name in enumerate(names):\n",
    "    if (i%500 == 0): print('processed: {}/{}'.format(i, len(names)))\n",
    "    arr = np.load(base_folder+name, allow_pickle=True)[65:97, 194:228]\n",
    "    np.save(save_folder+name, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(arr[80:112, 175:207])\n",
    "arr[80:112, 175:207].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取O3的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_lis(basePath):\n",
    "    \"\"\"\n",
    "    获得数据的名称列表\n",
    "    \"\"\"\n",
    "    names = os.listdir(basePath)\n",
    "    return names\n",
    "\n",
    "\n",
    "basePath = \"data/china_camx_original/\"\n",
    "names = get_name_lis(basePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def process_netCDF_get_O3():\n",
    "    \"\"\"获得O3的数据，保存在data_O3_numpy里面\"\"\"\n",
    "    basePath = \"data/china_camx_original/\"\n",
    "    for name in names:\n",
    "        path = basePath + name\n",
    "        file_obj = nc.Dataset(path)\n",
    "        O3 = file_obj.variables[\"O3\"][:]\n",
    "        O3_array = np.array([i[0] for i in O3])\n",
    "        #     print(O3_array.shape)\n",
    "\n",
    "\n",
    "#         np.save(\"data_O3_numpy/{}.npy\".format(name[39:47]), O3_array)\n",
    "# process_netCDF_get_O3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check data range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"data/pm25_hourly/\"\n",
    "res = []\n",
    "for name in os.listdir(base):\n",
    "    path = base + name\n",
    "    arr = np.load(path, allow_pickle=True)\n",
    "    stat = [arr.min(), round(arr.max(), 4), round(arr.mean(), 4)]\n",
    "    df = pd.DataFrame(arr.reshape((216 * 270, 1)))\n",
    "    stat.append(df[df > 0].count() / (216 * 270.0))\n",
    "    res.append(stat)\n",
    "#     print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cal max and min\n",
    "np.min([i[0] for i in res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot([i[0] for i in res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 12), dpi=150)\n",
    "plt.plot([i[1] for i in res])\n",
    "plt.xlabel(\"sample number\")\n",
    "plt.ylabel(\"max O3 value\")\n",
    "plt.title(\"max O3 value in the whole china\")\n",
    "plt.savefig(\"saved/max_o3_hourly_sample.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 12), dpi=150)\n",
    "plt.plot([i[2] for i in res])\n",
    "plt.xlabel(\"sample number\")\n",
    "plt.ylabel(\"mean O3 value\")\n",
    "plt.title(\"mean O3 value in the whole china\")\n",
    "plt.savefig(\"saved/mean_o3_hourly_check.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare with original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_distance_helper(lon1, lat1, lon2, lat2):  # 经度1，纬度1，经度2，纬度2 （十进制度数）\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "    输出：距离km\n",
    "    \"\"\"\n",
    "    # 将十进制度数转化为弧度\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine公式\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371  # 地球平均半径，单位为公里\n",
    "    return c * r\n",
    "\n",
    "\n",
    "def cal_distance(location1: tuple, location2: tuple) -> float:\n",
    "    return cal_distance_helper(location1[0], location1[1], location2[0], location2[1])\n",
    "\n",
    "\n",
    "def get_nearist_index(location_array: np.array, location: tuple) -> tuple:\n",
    "    \"\"\"\n",
    "    input: the array stored all the location; the location\n",
    "    output: the location index (i, j), not the (lon,lat)\n",
    "    \"\"\"\n",
    "    minDis, index = None, (0, 0)\n",
    "    for i in range(len(location_array)):\n",
    "        for j in range(len(location_array[0])):\n",
    "            dis = cal_distance(location, location_array[i][j])\n",
    "            if not minDis:\n",
    "                minDis = dis\n",
    "            if dis < minDis:\n",
    "                minDis = dis\n",
    "                index = (i, j)\n",
    "    return index\n",
    "\n",
    "\n",
    "def get_location_array(basePath):\n",
    "    # print(os.listdir())\n",
    "    nameList = os.listdir(basePath)\n",
    "    path = basePath + nameList[10]\n",
    "    file_obj = nc.Dataset(path)\n",
    "    lon = file_obj.variables[\"longitude\"][:]\n",
    "    lat = file_obj.variables[\"latitude\"][:]\n",
    "\n",
    "    res_arr = np.zeros((216, 270), dtype=\"object\")\n",
    "    for i in range(len(lon)):\n",
    "        for j in range(len(lon[0])):\n",
    "            res_arr[i][j] = (lon[i][j], lat[i][j])\n",
    "    return res_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_camx_station_data(loc: tuple, dataType=\"O3\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    input the station location (lon, lat), dataType = O3 or PM25\n",
    "    return the O3 dateframe of that station\n",
    "    \"\"\"\n",
    "    # loc -> index\n",
    "    basePath = \"data/china_camx_original/\"\n",
    "    arr = get_location_array(basePath)\n",
    "\n",
    "    loc = get_nearist_index(arr, loc)\n",
    "\n",
    "    base = \"data/{}_hourly/\".format(dataType)\n",
    "    names = os.listdir(base)\n",
    "\n",
    "    # get value from the index, add time info\n",
    "    res = []\n",
    "    for name in names:\n",
    "        path = base + name\n",
    "        arr = np.load(path, allow_pickle=True)\n",
    "        res.append(\n",
    "            [\n",
    "                int(name[:4]),\n",
    "                int(name[4:6]),\n",
    "                int(name[6:8]),\n",
    "                int(name[9 : name.find(\".\")]),\n",
    "                arr[loc[0]][loc[1]],\n",
    "            ]\n",
    "        )\n",
    "    # to dataframe and add timestemp\n",
    "    df = pd.DataFrame(res, columns=[\"year\", \"month\", \"day\", \"hour\", \"value\"])\n",
    "    time_list = []\n",
    "    for i, row in df.iterrows():\n",
    "\n",
    "        string = (\n",
    "            \"-\".join([str(int(i)) for i in [row.year, row.month, row.day]])\n",
    "            + \" \"\n",
    "            + str(int(row.hour))\n",
    "            + \":00\"\n",
    "        )\n",
    "        t = pd.Timestamp(string)\n",
    "        time_list.append(t)\n",
    "    df[\"pubtime\"] = time_list\n",
    "    return df\n",
    "\n",
    "\n",
    "df = get_camx_station_data((87.5801, 43.8303), \"O3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monitor_data(path):\n",
    "    \"\"\"\n",
    "    get the shanghai xuhui data\n",
    "    \"\"\"\n",
    "    df_real = pd.read_csv(path)\n",
    "    df_real[\"pubtime\"] = pd.to_datetime(df_real.pubtime)\n",
    "    return df_real\n",
    "\n",
    "\n",
    "path = \"data/xinjiang_wulumuqi_2019.csv\"\n",
    "df_real = get_monitor_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m = pd.merge(df_real, df, on=\"pubtime\")[[\"pubtime\", \"value\", \"o3\"]]\n",
    "\n",
    "df_m[\"value\"] = df_m[\"value\"].apply(lambda x: 1000 * 44.3 * x / 22.4)\n",
    "\n",
    "df_m.index = pd.DatetimeIndex(df_m[\"pubtime\"])\n",
    "df_d = df_m.resample(\"d\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station = \"shandong rizhao\"\n",
    "dataType = \"O3\"\n",
    "plt.figure(figsize=(12, 6), dpi=200)\n",
    "plt.plot(df_d)\n",
    "plt.title(\"{} {} compare\".format(station, dataType))\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"{} value\".format(dataType))\n",
    "plt.savefig(\"saved/{}_{}_compare.jpg\".format(station, dataType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_m.o3_value, df_m.o3, linewidths=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_merge.O3_value[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_merge.o3[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 看看经度的左右是否符合要求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/china_camx_original/\"\n",
    "file_obj = nc.Dataset(path+os.listdir(path)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(15.03, 55.88)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "lon = file_obj.variables[\"longitude\"][:]\n",
    "lat = file_obj.variables[\"latitude\"][:]\n",
    "round(lat.min(),2), round(lat.max(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "masked_array(data=[115.82800293, 116.03286743, 116.23760986, 116.44229126,\n",
       "                   116.64685059, 116.85128784, 117.05566406, 117.2598877 ,\n",
       "                   117.46405029, 117.66809082, 117.87200928, 118.07580566,\n",
       "                   118.27954102, 118.48312378, 118.68658447, 118.88995361,\n",
       "                   119.09320068, 119.29632568, 119.49932861, 119.70220947,\n",
       "                   119.90499878, 120.1076355 , 120.31015015, 120.51254272,\n",
       "                   120.71478271, 120.91693115, 121.118927  , 121.32080078,\n",
       "                   121.52252197, 121.72412109, 121.92559814, 122.12692261],\n",
       "             mask=False,\n",
       "       fill_value=1e+20)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "lon[65:97, 196:228][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(30.627870559692383, 31.495628356933594)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "np.median([np.min(i) for i in lat[65:97, 196:228]]), np.median([np.max(i) for i in lat[65:97, 196:228]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lat = lat[65:97, 196:228]\n",
    "lat_new = [_lat[:, i] for i in range(32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(28.328094482421875, 33.88136672973633)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "np.median([np.min(i) for i in lat_new]), np.median([np.max(i) for i in lat_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(115.8280029296875, 123.33212280273438)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "lon[65:97, 196:228].min(), lon[65:97, 196:228].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(27.86715316772461, 34.28531265258789)"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "lat[65:97, 196:228].min(), lat[65:97, 196:228].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_obj.variables.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = ['PNO3','PSO4','PNH4','POA','SOA1','SOA2',\\\n",
    "             'SOA3','SOA4','SOPA','SOPB','PEC','FPRM','FCRS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.zeros((24, 1, 216, 270))\n",
    "for name in name_list:\n",
    "    print(name)\n",
    "    array = array + np.array(file_obj.variables[name][:], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(np.load(\"data_O3_sample_numpy/20190104-sample.npy\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_obj.variables['PNO3'][:] + file_obj.variables['PSO4'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(np.load(\"data_O3_numpy/20190102.npy\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将O3的数据进行采样，获得采样的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_o3 = get_name_lis(\"data_O3_numpy/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_O3_data():\n",
    "    '''将数据按照10%进行采样，得到相应的sample的数据，没采样的部分设置为0\n",
    "    最后一天因为不够24hour，所以函数失败了'''\n",
    "    xIndex = [i for i in range(0, 216, 10)]\n",
    "    yIndex = [i for i in range(0, 270, 10)]\n",
    "\n",
    "    for name in names_o3[:]:\n",
    "        array_res = np.zeros((24, 216, 270))\n",
    "        base = \"data_O3_numpy/\"\n",
    "        array_24 = np.load(base+name)\n",
    "        \n",
    "    #     print(name)\n",
    "        for hour in range(array_24.shape[0]):\n",
    "            for x in xIndex:\n",
    "                for y in yIndex:\n",
    "                    array_res[hour][x][y] = array_24[hour][x][y]\n",
    "    #         plot_distribution(array_res[hour])\n",
    "        np.save(\"data_O3_sample_numpy/{}-sample.npy\".format(name[:name.find(\".\")]), array_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sample_O3_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(np.load(\"data_O3_sample_numpy/20191226-sample.npy\")[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 尝试构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, nc, ngf):\n",
    "        super(Generator,self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(nc,ngf,kernel_size=4,stride=2,padding=1),\n",
    "                                 nn.BatchNorm2d(ngf),\n",
    "                                 nn.LeakyReLU(0.2,inplace=True))\n",
    "        # 16 x 16 x 64\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(ngf,ngf*2,kernel_size=4,stride=2,padding=1),\n",
    "                                 nn.BatchNorm2d(ngf*2),\n",
    "                                 nn.LeakyReLU(0.2,inplace=True))\n",
    "        # 8 x 8 x 128\n",
    "        \n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(ngf*2,ngf*4,kernel_size=4,stride=2,padding=1),\n",
    "                                 nn.BatchNorm2d(ngf*4),\n",
    "                                 nn.LeakyReLU(0.2,inplace=True))\n",
    "        # 4 x 4 x 256                     \n",
    "        # 4 x 4 x 256\n",
    "        self.layer4 = nn.Sequential(nn.ConvTranspose2d(ngf*4,ngf*2,kernel_size=4,stride=2,padding=1),\n",
    "                                 nn.BatchNorm2d(ngf*2),\n",
    "                                 nn.ReLU())\n",
    "        # 8 x 8 x 128\n",
    "        self.layer5 = nn.Sequential(nn.ConvTranspose2d(ngf*2,ngf,kernel_size=4,stride=2,padding=1),\n",
    "                                 nn.BatchNorm2d(ngf),\n",
    "                                 nn.ReLU())\n",
    "        # 16 x 16 x 64\n",
    "        self.layer6 = nn.Sequential(nn.ConvTranspose2d(ngf,nc,kernel_size=4,stride=2,padding=1),\n",
    "                                 nn.Tanh())\n",
    "        # 32 x 32 x 1\n",
    "    def forward(self,_cpLayer):\n",
    "        out = self.layer1(_cpLayer)\n",
    "        print(out.shape)\n",
    "        out = self.layer2(out)\n",
    "        print(out.shape)\n",
    "        out = self.layer3(out)\n",
    "        print(out.shape)\n",
    "        out = self.layer4(out)\n",
    "        print(out.shape)\n",
    "        out = self.layer5(out)\n",
    "        print(out.shape)\n",
    "        out = self.layer6(out)\n",
    "        print(out.shape)\n",
    "        return out\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,nc,ndf):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.layer1_image = nn.Sequential(nn.Conv2d(nc,ndf//2,kernel_size=4,stride=2,padding=1),\n",
    "                                 #nn.BatchNorm2d(ndf/2),\n",
    "                                 nn.LeakyReLU(0.2,inplace=True))\n",
    "        # 16 x 16\n",
    "        self.layer1_cp = nn.Sequential(nn.Conv2d(nc,ndf//2,kernel_size=4,stride=2,padding=1),\n",
    "                                 #nn.BatchNorm2d(ndf/2),\n",
    "                                 nn.LeakyReLU(0.2,inplace=True))\n",
    "        # 16 x 16\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(ndf,ndf*2,kernel_size=4,stride=2,padding=1),\n",
    "                                 nn.BatchNorm2d(ndf*2),\n",
    "                                 nn.LeakyReLU(0.2,inplace=True))\n",
    "        # 8 x 8\n",
    "        \n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(ndf*2,ndf*4,kernel_size=4,stride=2,padding=1),\n",
    "                                 nn.BatchNorm2d(ndf*4),\n",
    "                                 nn.LeakyReLU(0.2,inplace=True))\n",
    "        # 4 x 4\n",
    "        \n",
    "        self.layer4 = nn.Sequential(nn.Conv2d(ndf*4,1,kernel_size=4,stride=1,padding=0),\n",
    "                                 nn.Sigmoid())\n",
    "        # 1\n",
    "#         self.layer5 = nn.Sequential(nn.Linear(, 1),\n",
    "#                                  nn.Sigmoid())\n",
    "        \n",
    "    def forward(self,dem,_cpLayer):\n",
    "        print(dem.shape)\n",
    "        out_1 = self.layer1_image(dem)\n",
    "        print(out_1.shape)\n",
    "        out_2 = self.layer1_cp(_cpLayer)        \n",
    "        out = self.layer2(torch.cat((out_1,out_2),1))\n",
    "        print(\"concat\", out.shape)\n",
    "        out = self.layer3(out)\n",
    "        print(out.shape)\n",
    "        out = self.layer4(out)\n",
    "        print(out.shape)\n",
    "#         out = self.layer5(out)\n",
    "#         print(out.shape)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = torch.tensor([[1,2,4],[2,5,3]])\n",
    "tmp, tmp.view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Discriminator(1, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = G.forward(img)\n",
    "res = D.forward(img, img)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(128, 20)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(1, 64)\n",
    "G = G.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.view(img.size()[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.load(\"data_O3_sample_numpy/20191226-sample.npy\")[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2 = arr[np.newaxis,np.newaxis,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.from_numpy(arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "img.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = res.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(plot[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0ed48c43e8e6e0227038ab82c29dc7753bd74e8f23d1929cc39f55c5c1d7c76e2",
   "display_name": "Python 3.8.5 64-bit ('machine_learning': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}